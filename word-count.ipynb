{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words in each period of time\n",
    "\n",
    "Before election date, on election date, after election date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from pythainlp import Tokenizer\n",
    "from pythainlp.corpus import thai_words, thai_stopwords\n",
    "from pythainlp import thai_digits, thai_punctuations, thai_vowels, thai_tonemarks, thai_symbols, thai_signs\n",
    "\n",
    "##############\n",
    "\n",
    "msg_file_name = \"edang-2019-01-21-2019-05-25.xlsx\"\n",
    "msg_sheet_name = \"twitter\"\n",
    "msg_sheet_before = \"before\"\n",
    "msg_sheet_during = \"during\"\n",
    "msg_sheet_after = \"after\"\n",
    "msg_column = \"Message\"\n",
    "msg_datetime_column = \"Post time\"\n",
    "\n",
    "keywords_file_name = \"edang-keywords.txt\"  # to add to tokenizer's dictionary\n",
    "additional_keywords = {\n",
    "    \"#พลังประชารัฐ\",\n",
    "    \"#อนาคตใหม่\"\n",
    "}\n",
    "\n",
    "#############\n",
    "\n",
    "# datetime format from Zocial Eye Excel export\n",
    "datetime_format = '%Y-%m-%d %H:%M:%S'  # 2019-02-15 15:50:56\n",
    "\n",
    "# the election day\n",
    "election_date = datetime.datetime(2019, 3, 24)\n",
    "\n",
    "# two day before election day announcement (2019-1-23)\n",
    "# = 62 days before the election day\n",
    "start_date = datetime.datetime(2019, 1, 21) \n",
    "\n",
    "# 62 days after the election day\n",
    "end_date = datetime.datetime(2019, 5, 25)\n",
    "\n",
    "#############\n",
    "\n",
    "# if a token only has these characters, not considered it as a word\n",
    "NON_WORD_CHARS = string.whitespace + string.punctuation + string.digits + thai_digits + thai_punctuations + thai_vowels + thai_tonemarks + thai_symbols + thai_signs\n",
    "\n",
    "# Words to be used as replacement\n",
    "# (normalization for better classification, we hope)\n",
    "REPLACE_LINK = \" NNLINK \"\n",
    "REPLACE_EMAIL = \" NNEMAIL \"\n",
    "REPLACE_HAHA = \" NNHAHA \"\n",
    "\n",
    "\n",
    "# <tag>, http://, www., .php, @mention, mail@address.com, hahaha, 555, 1234\n",
    "# To be normalized\n",
    "RE_HTTP_WWW = re.compile(r\"(?:\\b\\S{3,}:\\/{1,}\\S*)|(?:[wW]{2,}\\.\\S+)\")\n",
    "RE_EXT = re.compile(\n",
    "    r\"\\w+\\.(html|htm|shtm|shtml|cgi|php|php3|asp|aspx|cfm|cfml|jsp|png|gif|jpg|java|class|webp|mp3|mp4|mov|pl|do)(\\?\\S*)?\\b\",\n",
    "    flags=re.IGNORECASE,\n",
    ")\n",
    "RE_EMAIL = re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\")\n",
    "RE_HAHA = re.compile(r\"\\b(?:ha\\s*){2,}|\\u0E16{3,}|5{3,}(?!.\\d)\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "\n",
    "# Duplicated characters: aaaa, ! ! ! !, ะะะะะ\n",
    "RE_DUP_C_C = re.compile(r\"(?:^|\\s)(\\S{1,5})(\\s+\\1)+\")  # duplicated isolated chars/words\n",
    "RE_DUP_CC = re.compile(r\"(\\D{2,})\\1{3,}\")  # duplicated non-digits\n",
    "RE_DUP_C6 = re.compile(r\"(\\D)\\1{5,}\")  # duplicated non-digits (six or more characters)\n",
    "RE_DUP_THAI = re.compile(\n",
    "    r\"([\\u0E2F-\\u0E3A\\u0E3F\\u0E40-\\u0E4F\\?\\!])\\1+\"\n",
    ")  # Thai vowels/symbols\n",
    "\n",
    "\n",
    "def normalize_link(text: str, place_holder: str = REPLACE_LINK) -> str:\n",
    "    text = RE_HTTP_WWW.sub(place_holder, text)  # http, https, mailto, www.\n",
    "    text = RE_EXT.sub(place_holder, text)  # .html, php3, .jpg\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_email(text: str, place_holder: str = REPLACE_EMAIL) -> str:\n",
    "    text = RE_EMAIL.sub(place_holder, text)  # mail@address.com\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_haha(text: str, place_holder: str = REPLACE_HAHA) -> str:\n",
    "    text = RE_HAHA.sub(place_holder, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_dup_chars(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace duplicated characters\n",
    "    e.g.\n",
    "    righttttt -> rightttt\n",
    "    มรรรรรค -> มรรรรค\n",
    "    มาาาาาาาาาาาาาาาาาาาาาาากกกก -> มากกกก\n",
    "    อิอิอิอิอิอิ -> อิอิอิอิ\n",
    "    ! ! ! ! -> !\n",
    "    \"\"\"\n",
    "    text = RE_DUP_C_C.sub(r\"\\1\", text)\n",
    "    text = RE_DUP_CC.sub(r\"\\1\\1\", text)\n",
    "    text = RE_DUP_C6.sub(r\"\\1\\1\\1\\1\\1\", text)\n",
    "    text = RE_DUP_THAI.sub(r\"\\1\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = normalize_link(text)\n",
    "    text = normalize_email(text)\n",
    "    text = remove_dup_chars(text)\n",
    "    text = normalize_haha(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def is_word(word: str) -> bool:\n",
    "    if not word:\n",
    "        return False\n",
    "\n",
    "    for ch in word:\n",
    "        if ch not in NON_WORD_CHARS:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def combine_hashtag(in_tokens: List[str]) -> List[str]:\n",
    "    out_tokens = []\n",
    "\n",
    "    in_tokens_len = len(in_tokens)\n",
    "    i = 0\n",
    "    while i < in_tokens_len:\n",
    "        if in_tokens[i].lstrip() != \"#\":\n",
    "            out_tokens.append(in_tokens[i])\n",
    "        else:\n",
    "            hashtag = \"\"\n",
    "            while i+1 < in_tokens_len and in_tokens[i+1].strip() != \"\" and in_tokens[i+1].lstrip() != \"#\" and in_tokens[i+1].lstrip()[0] not in string.punctuation:\n",
    "                hashtag += in_tokens[i+1]\n",
    "                i = i+1\n",
    "            out_tokens.append(\"#\" + hashtag.strip())\n",
    "        i = i+1\n",
    "\n",
    "    return out_tokens\n",
    "\n",
    "\n",
    "keywords = set()\n",
    "with open(keywords_file_name, \"r\") as file:\n",
    "    for line in file.readlines():\n",
    "        keywords.add(line.strip())    \n",
    "print(keywords)\n",
    "wordlist = keywords.union(thai_words()).union(additional_keywords)\n",
    "\n",
    "tokenizer = Tokenizer(custom_dict=wordlist)\n",
    "del wordlist\n",
    "\n",
    "# word by day DataFrame\n",
    "dti = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "word_by_day = pd.DataFrame(index=dti, columns=[], data=0)\n",
    "word_by_day.index.name = \"Date\"\n",
    "word_by_day.columns.name = \"Word\"\n",
    "word_by_day.head()\n",
    "\n",
    "election_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_df = pd.read_excel(msg_file_name, sheet_name=msg_sheet_name, usecols=[msg_column, msg_datetime_column])\n",
    "\n",
    "# Use only date part, discards time\n",
    "msg_df[msg_datetime_column] = pd.to_datetime(msg_df[msg_datetime_column]).dt.normalize()\n",
    "msg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msg_df[msg_df[\"Post time\"] == election_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msg_df[msg_df[\"Post time\"] < election_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(msg_df[msg_df[\"Post time\"] > election_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "election_date_df = msg_df[msg_df[\"Post time\"] == election_date]\n",
    "\n",
    "stopwords = thai_stopwords()\n",
    "for index, row in election_date_df.iterrows():\n",
    "    msg = row[msg_column]\n",
    "    msg = normalize_text(msg)\n",
    "    words = combine_hashtag(tokenizer.word_tokenize(msg))\n",
    "    for word in words:\n",
    "        word = str(word.strip())\n",
    "        if is_word(word) and word not in stopwords:\n",
    "            if word not in word_by_day.columns:\n",
    "                word_by_day[word] = 0\n",
    "            _date = row[msg_datetime_column]\n",
    "            if _date not in word_by_day.index:\n",
    "                word_by_day.loc[_date] = 0\n",
    "            word_by_day.loc[_date][word] += 1\n",
    "\n",
    "word_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word-by-day-on-election-date.pkl\", \"wb\") as file:\n",
    "    pickle.dump(word_by_day, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_election_date_df = msg_df[msg_df[\"Post time\"] < election_date]\n",
    "\n",
    "for index, row in before_election_date_df.iterrows():\n",
    "    msg = row[msg_column]\n",
    "    msg = normalize_text(msg)\n",
    "    words = combine_hashtag(tokenizer.word_tokenize(msg))\n",
    "    for word in words:\n",
    "        word = str(word.strip())\n",
    "        if is_word(word) and word not in stopwords:\n",
    "            if word not in word_by_day.columns:\n",
    "                word_by_day[word] = 0\n",
    "            _date = row[msg_datetime_column]\n",
    "            if _date not in word_by_day.index:\n",
    "                word_by_day.loc[_date] = 0\n",
    "            word_by_day.loc[_date][word] += 1\n",
    "\n",
    "word_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word-by-day-before-election-date.pkl\", \"wb\") as file:\n",
    "    pickle.dump(word_by_day, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_election_date_df = msg_df[msg_df[\"Post time\"] > election_date]\n",
    "\n",
    "for index, row in after_election_date_df.iterrows():\n",
    "    msg = row[msg_column]\n",
    "    msg = normalize_text(msg)\n",
    "    words = combine_hashtag(tokenizer.word_tokenize(msg))\n",
    "    for word in words:\n",
    "        word = str(word.strip())\n",
    "        if is_word(word) and word not in stopwords:\n",
    "            if word not in word_by_day.columns:\n",
    "                word_by_day[word] = 0\n",
    "            _date = row[msg_datetime_column]\n",
    "            if _date not in word_by_day.index:\n",
    "                word_by_day.loc[_date] = 0\n",
    "            word_by_day.loc[_date][word] += 1\n",
    "\n",
    "word_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"word-by-day-after-election-date.pkl\", \"wb\") as file:\n",
    "    pickle.dump(word_by_day, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in word_by_day.iteritems():\n",
    "    if item.sum() < 100:\n",
    "        del word_by_day[item.name]\n",
    "\n",
    "word_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = []\n",
    "\n",
    "for index, item in word_by_day.iteritems():\n",
    "    word_freqs.append((item.name, item.sum()))\n",
    "\n",
    "def getKey(item):\n",
    "    return item[1]\n",
    "\n",
    "word_freqs = sorted(word_freqs, key=getKey, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
